{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detección de anomalías\n",
    "\n",
    "En este ejercicio vamos a trabajar en detectar anomalías. Para ello vamos a probar distintas técnicas como la transformación inversa de modelos de reducción de dimensionalidad o Isolation Forest.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import SparsePCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Los datos\n",
    "\n",
    "Vamos a trabajar con el dataset creditcard con el que trabajamos en el examen y en la entrega de los autoencoders. Como en los casos anteriores, lo primero que haremos será cargar el dataset guardar en una variable llamada X los atributos y en una variable Y las clases resultantes y lo vamos a escalar entre 0 y 1 los atributos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'examen-parcial-creditcard.csv'\n",
    "df=pd.read_csv(file_name)\n",
    "df=df.drop(columns=['Time'])\n",
    "X=df.copy().drop(columns=['Class'])\n",
    "Y=df['Class'].copy()\n",
    "scl=MinMaxScaler()\n",
    "X= pd.DataFrame(scl.fit_transform(X),columns=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación dividismos los sets  X e y cada uno de ellos en dos subsets, uno de entrenamiento y otro de validación. El de entrenamiento debería tener el 90% de los puntos y debería estar estratificado en función de los valores de Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(X,Y,\n",
    "                                                 test_size=0.1,\n",
    "                                                 random_state=42,\n",
    "                                                 shuffle=True,\n",
    "                                                 stratify=Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detección de anomalías usando PCA.\n",
    "\n",
    "Aparte de las metodologías ya testadas en el curso en referencia a clustering (distancia a los centroides en KMEANS y la búsqueda de los puntos considerados como ruido en DBSCAN) y a reducción de dimensionalidades (como la aplicación de la tercera entrega (SOM) y cuarta (autoencoders)), vamos a probar alguna técnica adicional. La primera será usando PCA. Probaremos a reducir la dimensionalidad y, posteriormente, la reconstrucción, etiquetando como sospechosos aquellos que presenten un error de reconstrucción mayor. \n",
    "\n",
    "Para ello vamos a hacer una reducción de dimensionalidades hasta 10 dimensiones desarrollando las PCAs con el set de entrenamiento. A continuación se transformará el set de test y se deshará la transformación. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca =  PCA(n_components=10).fit(x_train)\n",
    "x_test_pca = pca.transform(x_test)\n",
    "x_test_pca_inverse = pca.inverse_transform(x_test_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez deshecha la transformación, calcule el error de reconstrucción como la diferencia euclídea entre el vector orginal y el vector reconstruido. Guarde en un dataframe el error para cada vector de entrada y añada una columna con la verdadera clase del movimiento (si es fraudulento o no)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    28481.000000\n",
       "mean         0.077876\n",
       "std          0.046927\n",
       "min          0.022711\n",
       "25%          0.056153\n",
       "50%          0.069006\n",
       "75%          0.085668\n",
       "max          1.104067\n",
       "Name: error, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffs = x_test - x_test_pca_inverse\n",
    "df = pd.DataFrame(diffs)\n",
    "df.insert(0, 'Class', y_test, True)\n",
    "df['error'] = np.linalg.norm(diffs, axis=1)\n",
    "df['error'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probad con distintos valores de componentes principales. Y evaluad los errores de reconstrucción para diferente número de componentes principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error by class for 10 components: \n",
      "Class\n",
      "0    0.077163\n",
      "1    0.491996\n",
      "Name: error, dtype: float64\n",
      "Error by class for 15 components: \n",
      "Class\n",
      "0    0.043086\n",
      "1    0.345963\n",
      "Name: error, dtype: float64\n",
      "Error by class for 20 components: \n",
      "Class\n",
      "0    0.016815\n",
      "1    0.072929\n",
      "Name: error, dtype: float64\n",
      "Error by class for 25 components: \n",
      "Class\n",
      "0    0.007212\n",
      "1    0.045444\n",
      "Name: error, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for n_components in range(10, 30, 5):\n",
    "    pca =  PCA(n_components=n_components).fit(x_train)\n",
    "    x_test_pca = pca.transform(x_test)\n",
    "    x_test_pca_inverse = pca.inverse_transform(x_test_pca)\n",
    "\n",
    "    diffs = x_test - x_test_pca_inverse\n",
    "    df = pd.DataFrame(diffs)\n",
    "    df.insert(0, 'Class', y_test, True)\n",
    "    df['error'] = np.linalg.norm(diffs, axis=1)\n",
    "    print(f\"Error by class for {n_components} components: \")\n",
    "    print(df.groupby('Class')['error'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse PCA\n",
    "Realice un proceso similar pero con un PCA dispersos con alpha 0.0001 y 10 componentes. Desarrolle el modelo de PCA disperso con los datos de entrenamiento, transforme y deshaga la transformación del set de test y evalúe el error de reconstrucción para los datos de los movimientos legales y fraudulentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    28481.000000\n",
       "mean         0.077935\n",
       "std          0.046854\n",
       "min          0.023289\n",
       "25%          0.056230\n",
       "50%          0.069088\n",
       "75%          0.085721\n",
       "max          1.102811\n",
       "Name: error, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_components = 10\n",
    "alpha = 0.0001\n",
    "random_state = 42\n",
    "n_jobs = -1\n",
    "\n",
    "sparse_pca = SparsePCA(n_components=n_components, alpha=alpha, random_state=random_state, n_jobs=n_jobs).fit(x_train)\n",
    "\n",
    "x_test_sparse_pca = sparse_pca.transform(x_test)\n",
    "x_test_sparse_pca_inverse = sparse_pca.inverse_transform(x_test_sparse_pca)\n",
    "\n",
    "diffs = x_test - x_test_sparse_pca_inverse\n",
    "df = pd.DataFrame(diffs)\n",
    "df.insert(0, 'Class', y_test, True)\n",
    "df['error'] = np.linalg.norm(diffs, axis=1)\n",
    "df['error'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probad diferentes valores de alpha y del número de componentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error by class for 0.0001 alpha: \n",
      "Class\n",
      "0    0.077224\n",
      "1    0.490444\n",
      "Name: error, dtype: float64\n",
      "Error by class for 0.001 alpha: \n",
      "Class\n",
      "0    0.077224\n",
      "1    0.490444\n",
      "Name: error, dtype: float64\n",
      "Error by class for 0.01 alpha: \n",
      "Class\n",
      "0    0.077234\n",
      "1    0.489888\n",
      "Name: error, dtype: float64\n",
      "Error by class for 0.1 alpha: \n",
      "Class\n",
      "0    0.077281\n",
      "1    0.487665\n",
      "Name: error, dtype: float64\n",
      "Error by class for 1.0 alpha: \n",
      "Class\n",
      "0    0.077282\n",
      "1    0.487666\n",
      "Name: error, dtype: float64\n",
      "Error by class for 10.0 alpha: \n",
      "Class\n",
      "0    0.077282\n",
      "1    0.487666\n",
      "Name: error, dtype: float64\n",
      "Error by class for 100.0 alpha: \n",
      "Class\n",
      "0    0.210256\n",
      "1    0.743746\n",
      "Name: error, dtype: float64\n",
      "Error by class for 1000.0 alpha: \n",
      "Class\n",
      "0    0.210256\n",
      "1    0.743746\n",
      "Name: error, dtype: float64\n",
      "Error by class for 10000.0 alpha: \n",
      "Class\n",
      "0    0.210256\n",
      "1    0.743746\n",
      "Name: error, dtype: float64\n",
      "Error by class for 100000.0 alpha: \n",
      "Class\n",
      "0    0.210256\n",
      "1    0.743746\n",
      "Name: error, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for alpha in range(10):\n",
    "    alpha = 0.0001*10**alpha\n",
    "    sparse_pca = SparsePCA(n_components=n_components, alpha=alpha, random_state=random_state, n_jobs=n_jobs).fit(x_train)\n",
    "\n",
    "    x_test_sparse_pca = sparse_pca.transform(x_test)\n",
    "    x_test_sparse_pca_inverse = sparse_pca.inverse_transform(x_test_sparse_pca)\n",
    "\n",
    "    diffs = x_test - x_test_sparse_pca_inverse\n",
    "    df = pd.DataFrame(diffs)\n",
    "    df.insert(0, 'Class', y_test, True)\n",
    "    df['error'] = np.linalg.norm(diffs, axis=1)\n",
    "    print(f\"Error by class for {alpha} alpha: \")\n",
    "    print(df.groupby('Class')['error'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel PCA\n",
    "\n",
    "Realice un proceso similar con nu PCA kernelizado con un kernel de base radial y 10 componentes. Evalúe de la misma manera los errores de reconstrucción de los movimientos legales y los movimientos fraudulentos y evalúe si resulta útil para marcar movimientos como fraudulentos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class\n",
      "0    0.202546\n",
      "1    0.724815\n",
      "Name: error, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "n_components = 10\n",
    "kernel = 'rbf'\n",
    "gamma = None\n",
    "random_state = 42\n",
    "\n",
    "kernel_pca = KernelPCA(n_components=n_components, kernel=kernel, gamma=gamma, random_state=random_state, fit_inverse_transform=True)\n",
    "kernel_pca.fit(x_train[:2000])\n",
    "\n",
    "x_test_kernel_pca = kernel_pca.transform(x_test)\n",
    "x_test_kernel_pca_inverse = kernel_pca.inverse_transform(x_test_kernel_pca)\n",
    "\n",
    "diffs = x_test - x_test_kernel_pca_inverse\n",
    "df = pd.DataFrame(diffs)\n",
    "df.insert(0, 'Class', y_test, True)\n",
    "df['error'] = np.linalg.norm(diffs, axis=1)\n",
    "print(df.groupby('Class')['error'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isolation Forest\n",
    "\n",
    "A continuación use Isolation Forest para deteccióni de anomalías. Use 1000 árboles con un 0.5% de contaminación. Evalúe el score que le predice a los movimientos fraudulentos y a los legales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "0    0.004854\n",
       "1    0.510204\n",
       "Name: iso_for, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_estimators = 1000\n",
    "max_samples = 'auto'\n",
    "contamination = 0.005\n",
    "random_state = 42\n",
    "\n",
    "iso_for = IsolationForest(\n",
    "    n_estimators=n_estimators,\n",
    "    max_samples=max_samples,\n",
    "    contamination=contamination,\n",
    "    random_state=random_state\n",
    ").fit(x_train)\n",
    "\n",
    "y_test_iso_for = iso_for.predict(x_test)\n",
    "\n",
    "df = pd.DataFrame(x_test)\n",
    "df.insert(0, 'Class', y_test, True)\n",
    "df['iso_for'] = y_test_iso_for\n",
    "df['iso_for'] = df['iso_for'].map({1: 0, -1: 1})\n",
    "df.groupby('Class')['iso_for'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Outlier Factor\n",
    "\n",
    "Por último pruebe con LOF con 20 vecinos y un 0.5% de contaminación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       "0    0.004854\n",
       "1    0.102041\n",
       "Name: LOF, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_neighbors = 20\n",
    "contamination = 0.005\n",
    "\n",
    "LOF = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=contamination)\n",
    "LOF.fit(x_train)\n",
    "\n",
    "y_test_LOF = LOF.fit_predict(x_test)\n",
    "\n",
    "df = pd.DataFrame(x_test)\n",
    "df.insert(0, 'Class', y_test, True)\n",
    "df['LOF'] = y_test_LOF\n",
    "df['LOF'] = df['LOF'].map({1: 0, -1: 1})\n",
    "df.groupby('Class')['LOF'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
